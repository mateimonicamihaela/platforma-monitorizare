# Platforma de Monitorizare a Starii unui Sistem

## Scopul Proiectului
- [Descriere detaliata a scopului proiectului. ]

Acest proiect reprezintÄƒ o platformÄƒ completÄƒ de monitorizare È™i automatizare DevOps, dezvoltatÄƒ pentru a demonstra un flux de integrare continuÄƒ (CI/CD) È™i administrare a infrastructurii containerizate.

AplicaÈ›ia urmÄƒreÈ™te starea sistemului (sau a unui container), colectÃ¢nd periodic informaÈ›ii despre:
- utilizarea procesorului (CPU),
- memoria disponibilÄƒ,
- numÄƒrul de procese active,
- spaÈ›iul de stocare (disk usage),
- uptime, hostname, È™i alÈ›i parametri relevanÈ›i.

Datele sunt salvate Ã®ntr-un fiÈ™ier jurnal (system-state.log), care este actualizat la fiecare interval configurabil de timp.
Un al doilea serviciu, de backup automat, monitorizeazÄƒ modificÄƒrile fiÈ™ierului de log È™i creeazÄƒ copii de siguranÈ›Äƒ etichetate temporal, asigurÃ¢nd persistenÈ›a È™i trasabilitatea datelor Ã®n timp.

### Arhitectura proiectului

âš™ï¸ Arhitectura È™i componentele principale

1. Cele 2 scripturi

ğŸ“œ Scriptul de monitorizare (monitoring.sh)
- RuleazÄƒ periodic È™i scrie Ã®n fiÈ™ierul system-state.log informaÈ›ii despre starea sistemului.
- Intervalul este configurabil prin variabila de mediu INTERVAL (implicit 5 secunde).
- Poate fi executat atÃ¢t local, cÃ¢t È™i Ã®n container Docker.

ğŸ’¾ Scriptul de backup (backup.py)
- MonitorizeazÄƒ fiÈ™ierul system-state.log È™i efectueazÄƒ backup automat dacÄƒ detecteazÄƒ modificÄƒri.
- Copiile sunt salvate Ã®n directorul /data/backup/ È™i denumite dupÄƒ data È™i ora curentÄƒ.
- Include un mecanism de rotaÈ›ie automatÄƒ (È™terge backup-urile vechi, pÄƒstrÃ¢nd ultimele N fiÈ™iere).

2. ğŸ³ Docker
- Fiecare serviciu ruleazÄƒ Ã®n propriul container (monitorizare È™i backup).
- Volumele partajate (/data) permit comunicarea È™i partajarea logurilor Ã®ntre containere.
- ConfiguraÈ›ia este gestionatÄƒ central prin fiÈ™ierul docker-compose.yml.

3. â˜¸ï¸ Kubernetes
- AplicaÈ›ia este orchestratÄƒ Ã®ntr-un cluster Kubernetes.
- Un deployment ruleazÄƒ ambele containere Ã®n acelaÈ™i pod, alÄƒturi de un container NGINX care expune fiÈ™ierele de loguri.
- Include un HPA (Horizontal Pod Autoscaler) care scaleazÄƒ automat aplicaÈ›ia Ã®ntre 2 È™i 10 replici pe baza utilizÄƒrii CPU È™i memoriei.
- RuleazÄƒ Ã®ntr-un namespace dedicat: monitoring.

4. ğŸ§© Ansible
- AutomatizeazÄƒ instalarea È™i configurarea mediului de rulare (inclusiv Docker È™i Docker Compose).
- GestioneazÄƒ deploy-ul aplicaÈ›iei pe maÈ™ini remote (VM-uri dedicate).
- Include playbook-uri distincte pentru:
  - instalarea Docker (install_docker.yml),
  - rularea aplicaÈ›iei (deploy_platform.yml).

5. ğŸ—ï¸ Jenkins (CI/CD)
- IntegreazÄƒ pipeline-uri pentru:
  - build-ul imaginilor Docker;
  - push-ul cÄƒtre Docker Hub;
  - deploy automat Ã®n Kubernetes;
  - testarea aplicaÈ›iei È™i verificarea backup-urilor.

- Fiecare serviciu (monitoring È™i backup) are propriul pipeline Ã®n jenkins/pipelines/.

6. ğŸŒ Terraform
- DefineÈ™te infrastructura de bazÄƒ pentru rularea aplicaÈ›iei:
  - crearea VM-urilor,
  - configurarea reÈ›elei È™i volumelor persistente,
  - setarea backend-ului de stocare pentru starea Terraform.

- Permite aprovizionarea rapidÄƒ a mediilor de test, staging È™i producÈ›ie.


ğŸ“Š Beneficii È™i rezultate
- ğŸ” Monitorizare continuÄƒ È™i centralizatÄƒ a resurselor.
- ğŸ’¾ Backup automat, sigur È™i trasabil al datelor.
- ğŸ§± Containere uÈ™or portabile, integrate Ã®n pipeline-uri DevOps.
- â˜¸ï¸ Scalabilitate automatÄƒ prin Kubernetes HPA.
- âš™ï¸ Automatizare completÄƒ a instalÄƒrii È™i deploy-ului prin Ansible.
- ğŸš€ CI/CD configurat end-to-end cu Jenkins.
- â˜ï¸ InfrastructurÄƒ definitÄƒ ca cod (IaC) prin Terraform.

```bash
â”œâ”€â”€ ansible
â”‚   â”œâ”€â”€ inventory.ini
â”‚   â””â”€â”€ playbooks
â”‚       â”œâ”€â”€ deploy_platform.yml
â”‚       â””â”€â”€ install_docker.yml
â”œâ”€â”€ data
â”‚   â”œâ”€â”€ backup
â”‚   â”‚   â””â”€â”€ system-state-20251027-124842.log
â”‚   â””â”€â”€ system-state.log
â”œâ”€â”€ docker
â”‚   â”œâ”€â”€ backup
â”‚   â”‚   â””â”€â”€ Dockerfile
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â””â”€â”€ monitoring
â”‚       â””â”€â”€ Dockerfile
â”œâ”€â”€ imagini
â”‚   â””â”€â”€ jenkins-logo.png
â”œâ”€â”€ jenkins
â”‚   â””â”€â”€ pipelines
â”‚       â”œâ”€â”€ backup
â”‚       â”‚   â””â”€â”€ Jenkinsfile
â”‚       â””â”€â”€ monitoring
â”‚           â””â”€â”€ Jenkinsfile
â”œâ”€â”€ k8s
â”‚   â”œâ”€â”€ deployment.yaml
â”‚   â””â”€â”€ hpa.yaml
â”œâ”€â”€ README.md
â”œâ”€â”€ scripts
â”‚   â”œâ”€â”€ backup.py
â”‚   â””â”€â”€ monitoring.sh
â””â”€â”€ terraform
    â”œâ”€â”€ backend.tf
    â””â”€â”€ main.tf
```

Acest subpunct este BONUS.
- [Desenati in excalidraw sau in orice tool doriti arhitectura generala a proiectului si includeti aici poza cu descrierea pasilor]

- Acesta este un exemplu de inserare de imagine in README.MD. Puneti imagine in directorul de imagini si o inserati asa:

![Jenkins Logo](imagini/jenkins-logo.png)

Consultati si [Sintaxa Markdown](https://www.markdownguide.org/cheat-sheet/)

## Structura Proiectului
[Aici descriem rolul fiecarui director al proiectului. Descrierea trebuie sa fie foarte pe scurt la acest pas. O sa intrati in detalii la pasii urmatori.]
- `/scripts`: 
    - `monitoring.sh`: Un script shell care scrie intr-un fisier, la un interval de timp, informatii despre sistem (CPU, memorie, uptime, procese active, utilizare disk, retea).
    - `backup.py`: Un script Python care face backup la fiÈ™ierul de log, dacÄƒ acesta s-a modificat.
.
- `/docker`: [Descriere Dockerfiles È™i docker-compose.yml. Aici descrieti legatura dintre fiecare Dockerfile si scripturile de mai sus (vedeti comentariul din fiecare Dockerfile)]
- `/ansible`: [Descriere rolurilor playbook-urilor È™i inventory]
- `/jenkins`: [Descrierea rolului acestui director si a subdirectoarelor. Unde sunt folosite fisierele din acest subdirector.]
- `/terraform`: [Descriere rol fiecare fisier Terraform folosit]

## Setup È™i Rulare
- [InstrucÈ›iuni de setup local È™i remote. Aici trebuiesc puse absolut toate informatiile necesare pentru a putea instala si rula proiectul. De exemplu listati aici si ce tool-uri trebuiesc instalate (Ansible, SSH config, useri, masini virtuale noi daca este cazul, etc) pasii de instal si comenzi].
- [Cand includeti instructiuni folositi blocul de code markdown cu limbajul specific codului ]

ğŸ–¥ï¸ scripts/monitoring.sh

- Suprascrie fiÈ™ierul `system-state.log` 
- Perioada la care se printeaza in fisierul `system-state.log` este prin `export INTERVAL=5`

ğŸ’¾ scripts/backup.py

- CreeazÄƒ backup doar dacÄƒ fiÈ™ierul s-a modificat
- Numele backup-ului include data È™i ora
- Directorul de backup este configurabil cu `export BACKUP_DIR=backup`
- Logurile sunt clare È™i informative
- TrateazÄƒ toate excepÈ›iile fÄƒrÄƒ a se opri

âš™ï¸ Variabile de mediu utilizate Ã®n proiect

| VariabilÄƒ           | UtilizatÄƒ de  | Descriere                                                              | Valoare implicitÄƒ                                                         | Exemplu suprascriere                                         |
| ------------------- | ------------- | ---------------------------------------------------------------------- | ------------------------------------------------------------------------- | ------------------------------------------------------------ |
| **INTERVAL**        | monitoring.sh | Intervalul Ã®n secunde la care se colecteazÄƒ informaÈ›iile despre sistem | `5`                                                                       | `INTERVAL=2 ./scripts/monitoring.sh`                         |
| **OUT_FILE**        | monitoring.sh | Calea cÄƒtre fiÈ™ierul Ã®n care se scrie starea sistemului                | `./system-state.log` *(local)*<br>`/data/system-state.log` *(Docker/K8s)* | `OUT_FILE=./data/system-state.log ./scripts/monitoring.sh`   |
| **BACKUP_INTERVAL** | backup.py     | Intervalul Ã®n secunde la care se verificÄƒ modificarea logului          | `5`                                                                       | `BACKUP_INTERVAL=5 python3 scripts/backup.py`                |
| **SRC_FILE**        | backup.py     | Calea fiÈ™ierului `system-state.log` monitorizat pentru schimbÄƒri       | `./system-state.log` *(local)*<br>`/data/system-state.log` *(Docker/K8s)* | `SRC_FILE=./data/system-state.log python3 scripts/backup.py` |
| **BACKUP_DIR**      | backup.py     | Directorul Ã®n care sunt salvate copiile logului                        | `./backup` *(local)*<br>`/data/backup` *(Docker/K8s)*                     | `BACKUP_DIR=./data/backup python3 scripts/backup.py`         |
| **MAX_BACKUPS**     | backup.py     | NumÄƒrul maxim de backup-uri pÄƒstrate                                   | `10`                                                                      | `MAX_BACKUPS=5 python3 scripts/backup.py`                    |


Recomandare de rulare:

```bash
# Rulare monitorizare
cd ~/work/platforma-monitorizare/scripts/
chmod +x monitoring.sh
cd ~/work/platforma-monitorizare
export INTERVAL=5
export OUT_FILE=./data/system-state.log 
./scripts/monitoring.sh

# Rulare backup
cd ~/work/platforma-monitorizare
export BACKUP_INTERVAL=5
export SRC_FILE=./data/system-state.log 
export BACKUP_DIR=./data/backup
python3 scripts/backup.py
```


## Setup È™i Rulare Docker
In aceastÄƒ secÈ›iune este documentat modul de Ã®mpachetare È™i rulare a celor douÄƒ scripturi (monitorizare È™i backup) Ã®n imagini Docker separate.
AplicaÈ›ia poate rula atÃ¢t individual, cÃ¢t È™i Ã®mpreunÄƒ, folosind servicii Docker conectate printr-un volum comun.

3 secÈ›iuni pentru Docker:

1ï¸âƒ£ Build manual al imaginilor Docker

2ï¸âƒ£ Rulare individualÄƒ cu docker run

3ï¸âƒ£ Rulare orchestratÄƒ cu docker compose


ğŸ³ Rulare cu Docker (fÄƒrÄƒ Docker Compose)

âœ… 1) Construirea imaginilor Docker

```bash
cd ~/work/platforma-monitorizare/docker

docker build -t mateimonicamihaela/monitoring:latest \
  --file monitoring/Dockerfile ../

docker build -t mateimonicamihaela/backup:latest \
  --file backup/Dockerfile ../
```

âœ… 2) Rularea containerelor individual

â–¶ï¸ Container monitoring

Scrie system-state.log Ã®n volumul mapat local:

```bash
docker run -d \
  --name monitoring-container \
  -e INTERVAL=5 \
  -e OUT_FILE=/data/system-state.log \
  -v "$(pwd)"/../data:/data \
  mateimonicamihaela/monitoring:latest
```
â–¶ï¸ Containerul de backup

Face backup periodic doar dacÄƒ logul s-a modificat:

```bash
docker run -d \
  --name backup-container \
  -e BACKUP_INTERVAL=5 \
  -e SRC_FILE=/data/system-state.log \
  -e BACKUP_DIR=/data/backup \
  -v "$(pwd)"/../data:/data \
  mateimonicamihaela/backup:latest
```

AfiÈ™are log execuÈ›ie:
```bash
docker logs -f monitoring-container
docker logs -f backup-container
```

DupÄƒ cÃ¢teva secunde de rulare, verificÄƒm fisierele locale:
```bash
ls -lh ../data/
ls -lh ../data/backup/
```
ğŸ›‘ Oprire È™i È™tergere containere

```bash
docker stop monitoring-container backup-container
docker rm monitoring-container backup-container
```

âœ… 3) Rulare orchestratÄƒ cu Docker Compose

Pentru rularea completÄƒ a aplicaÈ›iei se foloseÈ™te un volum partajat È™i setÄƒri de reÈ›ea automate via Docker Compose.

Intram in folderul Docker:
```bash
cd ~/work/platforma-monitorizare/docker
```

Construim imaginile Docker:
```bash
docker compose build
```

Pornim serviciile in fundal:
```bash
docker compose up -d  
```

Verificam daca ambele containere ruleaza:
```bash
docker ps      
```

Vizualizam logurile aplicatiei:
```bash
docker compose logs -f       
```

Oprim containerele:
```bash
docker compose down                              
```

ğŸ”— Cum comunicÄƒ Ã®ntre ele containerele

Containerele nu comunicÄƒ prin reÈ›ea, ci prin volumul local montat:

| Container            | Scrie Ã®n                 | CiteÈ™te din              | Director local            |
| -------------------- | ------------------------ | ------------------------ | ------------------------- |
| `monitoring-service` | `/data/system-state.log` | â€”                        | `./data/system-state.log` |
| `backup-service`     | `/data/backup/`          | `/data/system-state.log` | `./data/backup/`          |

Astfel, backup-service vede fiÈ™ierul actualizat de monitoring-service È™i creeazÄƒ copii noi doar dacÄƒ fiÈ™ierul s-a modificat.

ğŸ§° Testare manualÄƒ

Putem verifica direct continutul din containere:

```bash
docker exec -it monitoring-service cat /data/system-state.log
docker exec -it backup-service ls /data/backup
```
â˜ï¸ (OpÈ›ional) Publicarea imaginilor Ã®n Docker Hub

DupÄƒ ce verificam cÄƒ totul funcÈ›ioneazÄƒ, rulam:
```bash
docker images
```

Avem doua imagini locale:
- docker-monitoring:latest â†’ monitoring
- docker-backup:latest â†’ backup

Le etichetam corect pentru Docker Hub È™i le dam push cu comenzile de mai jos:

(1) Autentificare o singurÄƒ datÄƒ (dacÄƒ nu eÈ™ti logat)
```bash
docker login
```

(2) Monitoring â†’ tag + push
```bash
docker tag docker-monitoring:latest mateimonicamihaela/monitoring:latest
docker push mateimonicamihaela/monitoring:latest
```

(3) Backup â†’ tag + push
```bash
docker tag docker-backup:latest mateimonicamihaela/backup:latest
docker push mateimonicamihaela/backup:latest
```


## Setup È™i Rulare in Kubernetes
- [Bonus: Adaugati si o diagrama cu containerele si setupul de Kubernetes] 

(1) PrecondiÈ›ii (Porneste Minikube + ActiveazÄƒ metrics-server (pentru HPA))
```bash
cd ~/work/platforma-monitorizare
minikube start
minikube addons enable metrics-server
kubectl get pods -A | grep metrics
```

DacÄƒ NU ai imaginile Ã®n Docker Hub:
```bash
# dupÄƒ ce ai fÄƒcut build local
minikube image load mateimonicamihaela/monitoring:latest
minikube image load mateimonicamihaela/backup:latest
```

(2) Namespace: aplicatia trebuie sa ruleze intr-un namespace cu numele monitoring

k8s/namespace.yaml
```bash
kubectl apply -f k8s/namespace.yaml
```

(3) ConfigMap Nginx (pentru listare /logs È™i redirect)
```bash
kubectl -n monitoring apply -f k8s/nginx-config.yaml
```

(4) Creati un deployment cu 2 replici ce ruleaza in acelasi pod ambele containere,plus un container nginx ce expunee fisierul de loguri de sistem --> 3 containere/pod + Service

```bash
kubectl -n monitoring apply -f k8s/deployment.yaml
```
(5) Adaugati un HPA pe baza de CPU È™i memorie configurat cu min replicas 2 si max replicas 10)
```bash
kubectl -n monitoring apply -f k8s/hpa.yaml
```

(6) Verificare rapidÄƒ & acces

```bash
kubectl -n monitoring get pods
kubectl -n monitoring get deploy,svc,hpa
kubectl -n monitoring describe hpa platforma-monitorizare-hpa
kubectl top pods -n monitoring   # necesitÄƒ metrics-server
```

(7) Acces la aplicaÈ›ie (Nginx care serveÈ™te logurile)

Varianta 1 - Deschide Ã®n browser (URL generat de Minikube):
```bash
minikube service -n monitoring platforma-monitorizare --url

# AcceseazÄƒ:
#   <URL>/logs/system-state.log 
#   <URL>/logs/backup/   (listÄƒ de fiÈ™iere; autoindex activ din ConfigMap) 
```
Varianta 2 - Port-forward (convenabil pentru demo)
```bash
kubectl -n monitoring port-forward svc/platforma-monitorizare 8080:80

# apoi Ã®n alt terminal:
curl http://localhost:8080/logs/system-state.log

# sau in browser:
http://localhost:8080/logs/system-state.log
```


(8) Vezi logurile din containere

Monitorizare:
```bash
kubectl -n monitoring logs deploy/platforma-monitorizare -c monitoring --tail=30 -f
```

Backup:
```bash
kubectl -n monitoring logs deploy/platforma-monitorizare -c backup --tail=30 -f
```

ğŸ”— Exemple de URL complet pentru accesarea aplicaÈ›iei:

Logul curent al sistemului:

http://192.168.49.2:32055/logs/system-state.log

Backup-urile efectuate

http://192.168.49.2:32055/logs/backup/


ğŸ§  Cum se verificÄƒ in terminal:

curl http://192.168.49.2:32055/logs/system-state.log

curl http://192.168.49.2:32055/logs/backup/


ğŸ§© Structura actualÄƒ a Pod-ului (din k8s/deployment.yaml)

Ãn Pod avem 3 containere care ruleazÄƒ Ã®mpreunÄƒ È™i partajeazÄƒ un volum /data comun:

| Container        | Rol                                                                                         | Porturi expuse         | PersistenÈ›Äƒ                         |
| ---------------- | ------------------------------------------------------------------------------------------- | ---------------------- | ----------------------------------- |
| ğŸ–¥ï¸ `monitoring` | ruleazÄƒ `monitoring.sh` â€“ colecteazÄƒ starea sistemului È™i scrie Ã®n `/data/system-state.log` | âŒ nu expune porturi    | âœ… scrie Ã®n `/data/system-state.log` |
| ğŸ§± `backup`      | ruleazÄƒ `backup.py` â€“ monitorizeazÄƒ fiÈ™ierul de log È™i face copii Ã®n `/data/backup/`        | âŒ nu expune porturi    | âœ… salveazÄƒ Ã®n `/data/backup/`       |
| ğŸŒ `nginx`       | serveÈ™te prin HTTP conÈ›inutul din `/data/` (loguri + backup-uri)                            | âœ… expune portul **80** | âœ… monteazÄƒ `/data` read-only        |

AlternativÄƒ completÄƒ â€” â€hard resetâ€ (dacÄƒ vrem sÄƒ cureÈ›am tot proiectul)
```bash
docker compose down --remove-orphans
docker container prune -f
docker image prune -f
docker volume prune -f
docker network prune -f
docker compose up -d
```


## Setup È™i Rulare in Ansible
- [Includeti aici pasii detaliati de configurat si rulat Ansible pe masina noua]
- [Descrieti cum verificam ca totul a rulat cu succes? Cateva comenzi prin care verificam ca Ansible a instalat ce trebuia]

(1) Bootstrap VM nou + user nou (o singurÄƒ datÄƒ)

 Pe masina remote (masina noua) adaugam un user nou si ii setam cheia de ssh 

CreeazÄƒ userul nou (ex: monitor) 
```bash
sudo adduser monitor
```

Adaugam userul monitor in userii cu drept de sudo
```bash
sudo usermod -aG sudo monitor
groups monitor
```

Adaugam userul monitor in lista de useri ce nu au nevoie de parola la sudo
```bash
cd /etc/sudoers.d/
echo "monitor ALL=(ALL) NOPASSWD:ALL" | sudo tee monitor-nopasswd
```

(monitor este userul pe care il foloseste Ansible sa faca ssh pe masina server)
```bash
su - monitor
```

Verificam ca putem face sudo fara parola
```bash
sudo ls
```

Adaugam cheia de ssh a userului monitor in masina remote. Atentie: trebuie sa fiti logati cu userul monitor cand rulati aceste comenzi
```bash
mkdir .ssh
touch ~/.ssh/authorized_keys
echo â€œcheie ssh publica de pe masina clientâ€ >> ~/.ssh/authorized_keys
cat ~/.ssh/authorized_keys
```

Instalam ssh server pe masina remote
```bash
sudo apt update
sudo apt install -y openssh-server
service ssh status
```

Luam IP-ul masinii remote (IP-ul care nu se termina in .1)
```bash
ip addr | grep 192.168
```

Ne afiseaza: 
```bash
monitor@baseline:~$ ip addr | grep 192.168
    inet 192.168.100.238/24 brd 192.168.100.255 scope global dynamic noprefixroute enp0s8
    inet 192.168.49.1/24 brd 192.168.49.255 scope global br-4ef4fc0cb34f
```

Revenim pe masina client (ubuntu2204) si incercam sa facem ssh cu userul monitor
```bash
ssh monitor@192.168.100.238
```

(2) Ansible pe maÈ™ina locala + inventory

Instalam pip pentru Python3
```bash
sudo apt update
sudo apt install -y python3-pip
pip3 --version
```

Instalam Ansible pe masina client (ubuntu2204).
```bash
sudo apt update
sudo apt install -y ansible 
ansible --version
```

Pe masina client (ubuntu2204) citim cheia publica a userului curent
```bash
cat ~/.ssh/id_rsa.pub
```

Revenim pe masina client (ubuntu2204) si incercam sa facem ssh cu userul monitor
```bash
ssh monitor@192.168.100.238
```
AsigurÄƒ-te cÄƒ existÄƒ Python 3 pe VM (Ansible are nevoie)
```bash
sudo apt-get update
sudo apt-get install -y python3
```
Intoarce-te la userul eu
```bash
exit
```

ansible/ansible.cfg

ansible/inventory.ini (actualizeazÄƒ IP-ul!)

ansible/requirements.yml

InstaleazÄƒ colecÈ›ia pe maÈ™ina de control:

```bash
cd ansible
ansible-galaxy collection install -r requirements.yml
```

Test ping simplu
```bash
ansible monitoring_vm -m ping
```


(3) Playbook 1 â€” InstaleazÄƒ Docker (Docker CE + compose plugin)

ansible/playbooks/install_docker.yml

RuleazÄƒ:
```bash
cd ansible
ansible-playbook playbooks/install_docker.yml
```


(4) Playbook 2 â€” Deploy cu docker compose + verificÄƒri log & backup

Acest playbook:

- cloneazÄƒ repo-ul meu pe VM Ã®n /opt/platforma-monitorizare

- ruleazÄƒ docker compose up -d din docker/

- aÈ™teaptÄƒ sÄƒ aparÄƒ system-state.log

- verificÄƒ faptul cÄƒ s-a creat cel puÈ›in un fiÈ™ier Ã®n data/backup/


ansible/playbooks/deploy_platform.yml

```bash
ansible-playbook playbooks/deploy_platform.yml
```

VerificÄƒri manuale: 

Pe masina remote cu userul nou

```bash
ssh monitor@192.168.100.238
docker ps --format 'table {{.Names}}\t{{.Image}}\t{{.Status}}'
sudo ls -lh /opt/platforma-monitorizare/data
sudo ls -lh /opt/platforma-monitorizare/data/backup
sudo tail -n 20 /opt/platforma-monitorizare/data/system-state.log
```
Pe masina locala

```bash
ansible monitoring_vm -m command -a "docker ps"
```



## Jenkins CI/CD È™i Automatizari
- [Descriere pipeline-uri Jenkins. Puneti aici cat mai detaliat ce face fiecare pipeline de jenkins cu poze facute la pipeline in Blue Ocean. Detaliati cat puteti de mult procesul de CI/CD folosit.]
- [Detalii cu restul cerintelor de CI/CD (cum ati creat userul nou ce are access doar la resursele proiectului, cum ati creat un View now pentru proiect, etc)]
- [Daca ati implementat si punctul E optional atunci detaliati si setupul de minikube.]


## Terraform È™i AWS
- [Prerequiste]
- [InstrucÈ›iuni pentru rularea Terraform È™i configurarea AWS]
- [Daca o sa folositi pentru testare localstack in loc de AWS real puneti aici toti pasii pentru install localstack.]
- [Adaugati instructiunile pentru ca verifica faptul ca Terraform a creat corect infrastructura]

## Depanare si investigarea erorilor
- [Descrieti cum putem accesa logurile aplicatiei si cum ne logam pe fiecare container pentru eventualele depanari de probleme]
- [Descrieti cum ati gandit logurile (formatul logurilor, levelul de log)]


## Resurse
- [Listati aici orice link catre o resursa externa il considerti relevant]
- Exemplu de URL:
- [Sintaxa Markdown](https://www.markdownguide.org/cheat-sheet/)
- [Schelet Proiect](https://github.com/amihai/platforma-monitorizare)
